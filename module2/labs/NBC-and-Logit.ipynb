{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validate $\\rightarrow$ Train, Test \n",
    "### Focus: Naive Bayes Classifier & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When constructing a model, data availability may become an issue. \n",
    "In order to avoid overfitting, it is necessary to withhold some portion of the data as a test set. \n",
    "However, overfitting *on the test set* may also occur without a secondary validation step. \n",
    "As such, `scikit` contains a number of methods for cross-validation of data.\n",
    "\n",
    "## References\n",
    "1. [Scikit documentation - GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from collections import OrderedDict\n",
    "\n",
    "# load dataset \n",
    "# raw = load_iris()\n",
    "\n",
    "# X = raw.data # slice off only the first feature (.data is multi-dimensional)\n",
    "# y = raw.target # the target data is a single label, so it can all be kept\n",
    "\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)\n",
    "X = dataset.iloc[:, [1,2,6,9,10]]\n",
    "y = dataset.quality\n",
    "\n",
    "# test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# we'll use the Gaussian Naive Bayes classifier\n",
    "baseline_classifier = DummyClassifier()\n",
    "nb_classifier = GaussianNB()\n",
    "regression_classifier = LogisticRegression(max_iter = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Though a manual CV workflow was described in [the cross-validation lab](./CrossValidation.ipynb), the automated `cross_val_score()` will work well enough for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline score  [0.4296875  0.4296875  0.4296875  0.43359375 0.43529412]\n",
      "NB score : [0.61328125 0.578125   0.5859375  0.5859375  0.57647059]\n",
      "Regression score:  [0.640625   0.58203125 0.5546875  0.58203125 0.60784314]\n"
     ]
    }
   ],
   "source": [
    "# automated CV step\n",
    "baseline_scores = cross_val_score(baseline_classifier, X_train, y_train, cv=5)\n",
    "nb_scores = cross_val_score(nb_classifier, X_train, y_train, cv=5)\n",
    "regression_scores = cross_val_score(regression_classifier, X_train, y_train, cv=5)\n",
    "print(\"baseline score \", baseline_scores) # TODO: visualization of CV process\n",
    "print(\"NB score :\", nb_scores)\n",
    "print(\"Regression score: \", regression_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are performing cross validation with the training set. These cross-validation values represent how well (with 1 being a perfect score) the model performed against a small, as-yet-untrained portion of the data for the classification task.\n",
    "\n",
    "## Training the new models\n",
    "\n",
    "Since the CV values are relatively high, we can create a model using all the data in the training set and test against the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit new model\n",
    "baseline_classifier.fit(X_train, y_train)\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "regression_classifier.fit(X_train, y_train)\n",
    "\n",
    "# model.predict() returns class labels (integers)\n",
    "y_pred_baseline = baseline_classifier.predict(X_test)\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "y_pred_reg = regression_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score comparision between the three models. \n",
    "Score shows that bayes classifier is better that baseline. \n",
    "Again Regression is better in score than Bayes classifier. \n",
    "\n",
    "For comparision we used all the given features. But you can try out several amount of features to test the models. \n",
    "if we increase the number of features then accuracy will increase. but it will require more computation to converge. \n",
    "Again reduction of number of features will loose accuracy. We need to trade-off between computation power and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Metrics  Baseline  Naive Bayes  Logistic Regression\n",
      "Precision      0.16         0.53                 0.56\n",
      "   Recall      0.40         0.55                 0.58\n",
      "       F1      0.23         0.54                 0.55\n",
      "   Recall      0.40         0.55                 0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score\n",
    "from pandas import DataFrame\n",
    "from IPython.display import display \n",
    "\n",
    "data_dict = {\n",
    "    'Metrics': [\"Precision\", 'Recall', 'F1', 'Recall'],\n",
    "    'Baseline': [\n",
    "        precision_score(y_test, y_pred_baseline, average= 'weighted'),\n",
    "        recall_score(y_test, y_pred_baseline, average='weighted'),\n",
    "        f1_score(y_test, y_pred_baseline, average='weighted'),\n",
    "        accuracy_score(y_test, y_pred_baseline)\n",
    "    ],\n",
    "    \"Naive Bayes\": [\n",
    "        precision_score(y_test, y_pred_nb, average= 'weighted'),\n",
    "        recall_score(y_test, y_pred_nb, average='weighted'),\n",
    "        f1_score(y_test, y_pred_nb, average='weighted'),\n",
    "        accuracy_score(y_test, y_pred_nb)\n",
    "    ],\n",
    "    \"Logistic Regression\": [\n",
    "        precision_score(y_test, y_pred_reg, average= 'weighted'),\n",
    "        recall_score(y_test, y_pred_reg, average='weighted'),\n",
    "        f1_score(y_test, y_pred_reg, average='weighted'),\n",
    "        accuracy_score(y_test, y_pred_reg)\n",
    "    ]\n",
    "}\n",
    "table = np.around(DataFrame(data_dict), 2)\n",
    "print(table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "* The naive Bayes classifier and logistic regression perform equally well\n",
    "* Warning is thrown as for some of the classes precision is 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
