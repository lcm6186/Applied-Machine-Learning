{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Feature Selection\n",
    "\n",
    "In this lab you will learn about **feature selection**, \n",
    "which reduces the dimensionality of data for the following reasons:\n",
    "\n",
    "1. Reduces overfitting by removing noise introduced by some of the features.\n",
    "2. Reduces training time, which allows you to experiment more with different models and hyperparameters.\n",
    "3. Reduces data acquisition requirements.\n",
    "4. Improves comprehensibility of the model because a smaller set of features is more comprehendible to humans. That will enable you to focus on the main sources of predictability, make the model more justifiable to another person.\n",
    "\n",
    "For this session, we are going to use **red wine quality** dataset as a starting point for learning feature selection,\n",
    "and then select the most relevant feature for predicting wine quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feature selection methods generally falls into two categories known as **wrapper methods** and **filter methods**. There is a 3rd category, **embedded methods**, which includes those models where features are identified while learning the model parameters. In this notebook we will discuss wrapper and filter methods.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<span style=\"text-decoration: underline;\">**Wrapper methods**</span> use a subset of features.\n",
    "Based on the results drawn from the previous model trained on that subset of features, \n",
    "they are either added or removed from the subset.\n",
    "The problem is essentially reduced to a search problem.\n",
    "[Greedy algorithms](https://en.wikipedia.org/wiki/Greedy_algorithm) \n",
    "are the most desirable in multivariate feature selection scenario because 1) \n",
    "wrapper methods are usually computationally very expensive; 2) \n",
    "greedy algorithms don't necessary provide the optimal solution,\n",
    "which is good becuase it makes them less prone to overfitting.\n",
    "\n",
    "\n",
    "<span style=\"text-decoration: underline;\">**Filter methods**</span> apply a statistical measure and assign a score to each feature one at a time.\n",
    "In this lab, you will go through **Pearson's χ²** and **ANOVA F-value based feature selection** in this section. \n",
    "\n",
    "sklearn API reference:\n",
    "\n",
    "+ [sklearn.feature_selection.SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)\n",
    "+ [sklearn.feature_selection.chi2](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html)\n",
    "+ [sklearn.feature_selection.f_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html)\n",
    "+ [sklearn.feature_selection.mutual_info_classif](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html)\n",
    "+ [sklearn.feature_selection.RFE](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Uncomment following line to view original output.\n",
    "# np.random.seed(18937)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset location\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "assert os.path.exists(DATASET)\n",
    "\n",
    "# Load and shuffle\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store features and labels into variables **X** and **y** respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = dataset.iloc[:, :-1]\n",
    "# y = dataset.quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dataset.iloc[:, :-1])\n",
    "y = np.array(dataset.quality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection solution space\n",
    "\n",
    "From algorithm analysis point of view, a solution to feature selection problems can be respresented as a boolean vector,\n",
    "each component indicating whether the corresponding feature has been selected.\n",
    "For instance,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.array([False, True, True, False, False, True, True, False, False, False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sci-kit learn calls the corresponding indices to feature columns selected \"support\", \n",
    "which can be obtained using [np.flatnonzero()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.flatnonzero.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  5  6 10]\n"
     ]
    }
   ],
   "source": [
    "support = np.flatnonzero(selected)\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus a naive approach that exhaustively search all subsets of features would have to verify $2^p$ solutions for $p$ features,\n",
    "which translates into $\\Omega(2^p)$ [time complexity](https://en.wikipedia.org/wiki/Time_complexity).\n",
    "That is to say it would be very inefficient in practice.\n",
    "\n",
    "However, we will run an exhaustive search for all solutions that provide 5 features to establish a baseline.\n",
    "This limits time complexity to $O(p^5 \\cdot n)$, assuming scoring a model takes linear time $O(n)$.\n",
    "Therefore, the following cell checks all $\\begin{pmatrix} 11 \\\\ 5 \\end{pmatrix} = \\frac{11\\times10\\times9\\times8\\times7}{5\\times4\\times3\\times2\\times1}=462$ solutions, \n",
    "and displays top 3 subset of features ranked by accuracy.\n",
    "\n",
    "In Part 1 \"Wrapper methods\", we will use these solutions as comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.35149423850184036, (1, 4, 6, 9, 10)),\n",
       " (0.34831403567393326, (1, 4, 8, 9, 10)),\n",
       " (0.3469578882102963, (1, 6, 8, 9, 10))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def search_combinations(estimator, X, y, k=5):\n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # enumerate all combinations of 5 features\n",
    "    for subset in itertools.combinations(range(X.shape[1]), 5):\n",
    "        yield score(X[:, subset]), subset\n",
    "        \n",
    "sorted(search_combinations(LinearRegression(), X, y), reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Wrapper methods\n",
    "\n",
    "Withing wrapper methods, there are different strategies for selecting the features. Here we will learn about **forward selection**, **backward elimination** and **recursive feature elimination** strategies.\n",
    "\n",
    "### A. Forward selection\n",
    "\n",
    "Forward selection is an iterative method in which we start with having no feature in the model. \n",
    "In each iteration, we keep adding the feature which best improves our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "%accuracy if adding column:\n",
      "    {0: 1, 1: 15, 2: 5, 3: 0, 4: 1, 5: 0, 6: 3, 7: 3, 8: 0, 9: 6, 10: 22}\n",
      "add column 10\n",
      "\n",
      "%accuracy if adding column:\n",
      "    {0: 25, 1: 31, 2: 25, 3: 22, 4: 22, 5: 22, 6: 23, 7: 23, 8: 25, 9: 26}\n",
      "add column 1\n",
      "\n",
      "%accuracy if adding column:\n",
      "    {0: 32, 2: 31, 3: 31, 4: 31, 5: 31, 6: 32, 7: 31, 8: 32, 9: 33}\n",
      "add column 9\n",
      "\n",
      "%accuracy if adding column:\n",
      "    {0: 33, 2: 33, 3: 33, 4: 34, 5: 33, 6: 34, 7: 33, 8: 33}\n",
      "add column 6\n",
      "\n",
      "%accuracy if adding column:\n",
      "    {0: 34, 2: 34, 3: 34, 4: 35, 5: 34, 7: 34, 8: 34}\n",
      "add column 4\n",
      "[1, 4, 6, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "def forward_select(estimator, X, y, k=5):\n",
    "    # this array holds indicators of whether each feature is currently selected\n",
    "    selected = np.zeros(X.shape[1]).astype(bool)\n",
    "    \n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)  # accurary is the measure\n",
    "    \n",
    "    # find indices to selected columns\n",
    "    selected_indices = lambda: list(np.flatnonzero(selected))\n",
    "    \n",
    "    # repeat till k features are selected\n",
    "    while np.sum(selected) < k:\n",
    "        # indices to unselected columns\n",
    "        rest_indices = list(np.flatnonzero(~selected))\n",
    "    \n",
    "        # compute model scores with an additional feature\n",
    "        scores = [score(X[:, selected_indices() + [i]]) for i in rest_indices]\n",
    "        print('\\n%accuracy if adding column:\\n   ',\n",
    "              {i:int(s*100) for i,s in zip(rest_indices,scores)})\n",
    "        \n",
    "        # find index within `rest_indices` that points to the most predictive feature not yet selected \n",
    "        idx_to_add = rest_indices[np.argmax(scores)]\n",
    "        print('add column', idx_to_add)\n",
    "        \n",
    "        # select this new feature\n",
    "        selected[idx_to_add] = True\n",
    "        \n",
    "    return selected_indices()\n",
    "\n",
    "support = sorted(forward_select(LinearRegression(), X, y))\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Backward elimination\n",
    "\n",
    "In backward elimination, \n",
    "we start with all the features and remove the _least significant_ feature at each iteration,\n",
    "which improves the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "%accuracy if removing column:\n",
      "    {0: 36, 1: 32, 2: 35, 3: 36, 4: 35, 5: 35, 6: 35, 7: 36, 8: 35, 9: 33, 10: 31}\n",
      "remove column 7\n",
      "\n",
      "%accuracy if removing column:\n",
      "    {0: 36, 1: 32, 2: 35, 3: 36, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 0\n",
      "\n",
      "%accuracy if removing column:\n",
      "    {1: 32, 2: 35, 3: 35, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 3\n",
      "\n",
      "%accuracy if removing column:\n",
      "    {1: 32, 2: 35, 4: 35, 5: 35, 6: 35, 8: 35, 9: 33, 10: 24}\n",
      "remove column 2\n",
      "\n",
      "%accuracy if removing column:\n",
      "    {1: 31, 4: 34, 5: 35, 6: 34, 8: 35, 9: 33, 10: 24}\n",
      "remove column 5\n",
      "\n",
      "%accuracy if removing column:\n",
      "    {1: 31, 4: 34, 6: 34, 8: 35, 9: 33, 10: 23}\n",
      "remove column 8\n",
      "[1, 4, 6, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "def backward_eliminate(estimator, X, y, k=5):\n",
    "    # this array holds indicators of whether each feature is currently selected\n",
    "    selected = np.ones(X.shape[1]).astype(bool)\n",
    "    \n",
    "    # fit and score model based on some subset of features\n",
    "    score = lambda X_features: clone(estimator).fit(X_features, y).score(X_features, y)\n",
    "    \n",
    "    # find indices to selected columns\n",
    "    selected_indices = lambda: list(np.flatnonzero(selected))\n",
    "    \n",
    "    # repeat till k features are selected\n",
    "    while np.sum(selected) > k:\n",
    "        # Compute model scores with one of the features removed\n",
    "        scores = [score(X[:, list(set(selected_indices()) - {i})]) for i in selected_indices()]\n",
    "        print('\\n%accuracy if removing column:\\n   ',\n",
    "              {i:int(s*100) for i,s in zip(selected_indices(), scores)})\n",
    "        \n",
    "        # Find index that points to the least predictive feature\n",
    "        idx_to_remove = selected_indices()[np.argmax(scores)]\n",
    "        print('remove column', idx_to_remove)\n",
    "        \n",
    "        # Remove this feature\n",
    "        selected[idx_to_remove] = False\n",
    "        \n",
    "    return selected_indices()\n",
    "\n",
    "support = sorted(backward_eliminate(LinearRegression(), X, y))\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Recursive feature elimination\n",
    "\n",
    "**Recursive Feature elimination** is an even more greedy algorithm provided by sklearn, \n",
    "which finds good performing feature subset with high efficiency. This method has similarity with backward elimination technique. \n",
    "\n",
    "The importance of each feature is obtained either through a **`coef_`** attribute \n",
    "or through a **`feature_importances_`** attribute.\n",
    "So in order for recursive feature elimination algorithm in sklearn to work, \n",
    "the model is required to provide either of these attributes.\n",
    "\n",
    "Usually, we start off using a low complexity model and use it as a benchmark for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 5\n",
      "Selected Features: [1 4 7 8 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py:72: FutureWarning: Pass n_features_to_select=5 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "selector = RFE(model, 5)\n",
    "selector.fit(X, y)\n",
    "print(\"Num Features:\", selector.n_features_)\n",
    "print(\"Selected Features:\", np.flatnonzero(selector.support_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can transform dataset to include only these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 5)\n"
     ]
    }
   ],
   "source": [
    "X_new = selector.transform(X)\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Filter methods\n",
    "\n",
    "In filter methods, features are ranked in terms of predictiveness one by one, \n",
    "as opposed to considering a subset.\n",
    "They incorporate statistical or information theoretic measures to rank each feature instead of measuring accuracy of a model trained on selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Pearson's χ² test based feature selection\n",
    "\n",
    "The following cell shows the usage of a feature selector based on Pearson's χ² test.\n",
    "This constructs the approximate χ² distribution and scores each feature vs \n",
    "the label in order to determine which feature is more relevant, \n",
    "one at a time, then selects features according to the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "χ² statistic [1.12606524e+01 1.55802891e+01 1.30256651e+01 4.12329474e+00\n",
      " 7.52425579e-01 1.61936036e+02 2.75555798e+03 2.30432045e-04\n",
      " 1.54654736e-01 4.55848775e+00 4.64298922e+01]\n",
      "Selected indices [ 1  2  5  6 10]\n"
     ]
    }
   ],
   "source": [
    "selector = SelectKBest(chi2, k=5)\n",
    "selector.fit(X, y)\n",
    "print('χ² statistic', selector.scores_)\n",
    "print('Selected indices', selector.get_support(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the **transform()** method to select those feature columns from dataset and \n",
    "store into a new variable **X_selected**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_selected = selector.transform(X)\n",
    "X_selected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**selector.transform()** does the same as slicing out these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(X_selected, X[:, [1, 2, 5, 6, 10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will take a closer look at this procedure by implementing one so we can better distinguish different feature selection methods. \n",
    "χ² test has many applications.\n",
    "For feature selection, we utilize χ² statistic to test for dependence of each feature towards determining label.\n",
    "\n",
    "**Note:** _You do NOT need to learn by heart the details of the computation to work on practices._\n",
    "\n",
    "#### Step 1: Encode labels into orthogonal vector space\n",
    "\n",
    "This is also know as **one-hot encoding**, which is applicable to classification problems.\n",
    "Consider an example where you have defined 3 categories for possible outcomes: A, B and C.\n",
    "In order for machine learning algorithms to be able to handle this type of data,\n",
    "we have to convert them into numbers.\n",
    "One-hot encoding uses a vector $ (y_1, y_2, y_3) $ where \n",
    "$$ y_i = \\left[ \\text{result falls into i}^{th}\\text{ category} \\right] \\in \\left\\{ 0, 1 \\right\\} $$\n",
    "Therefore, one-hot encoding for A, B and C categories becomes (1, 0, 0), (0, 1, 0) and (0, 0, 1) respectively.\n",
    "This has an advantage over plainly translating A, B and C into 1, 2 and 3 (a.k.a **sparse encoding**)\n",
    "in a way that orthogonal vectors do not impose assumptions of their order or magnitudes between categories like numbers would.\n",
    "\n",
    "For example, 3>1 is true, how ever it doesn't mean to imply C>A or C is superior to A in any way.\n",
    "However this would affect the model's numerical stability.\n",
    "\n",
    "Therefore one-hot encoding is an widely adopted technique for processing categories. \n",
    "Sparse encoding could be used when persisting a dataset in order to save storage space.\n",
    "\n",
    "**Note:** If you recall _regression in R_ from 8610 (Stat/Math), \n",
    "one-hot encoding is how the categorical / nominal variables are encoded as \n",
    "independent predictors in the regression formula.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 6)\n",
      "[[0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "Y = np.array(LabelBinarizer().fit_transform(y))\n",
    "print(Y.shape)\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compute the [contingency table](https://en.wikipedia.org/wiki/Contingency_table) of observed frequencies\n",
    "\n",
    "**observed** is a (#classes)-by-(#features) matrix that contains the \"number of occurrences\" for each combination of feature and classes.\n",
    "\n",
    "**Note:** You previously saw contingency tables in the 8610 (Stat/Math) class and computed the Chi-Squared (χ²) statistic there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed = np.dot(Y.T, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Compute the expected frequencies using marginal frequencies\n",
    "\n",
    "**expected** has the same shape as **observed** matrix, but represent the expected frequencies in theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = np.dot(\n",
    "    Y.mean(axis=0).reshape(1, -1).T, # Mean value for all classes (transposed)\n",
    "    X.sum(axis=0).reshape(1, -1) # Marginal frequencies for all features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Compute the χ² statistic between **observed** and **expected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.12606524e+01 1.55802891e+01 1.30256651e+01 4.12329474e+00\n",
      " 7.52425579e-01 1.61936036e+02 2.75555798e+03 2.30432045e-04\n",
      " 1.54654736e-01 4.55848775e+00 4.64298922e+01]\n"
     ]
    }
   ],
   "source": [
    "chi_squared = np.sum((observed-expected)**2 / expected, axis=0)\n",
    "print(chi_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with sklearn **chi2()**, which returns two arrays containing χ² statistic and p-value, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Squared Statistic\n",
      "[1.12606524e+01 1.55802891e+01 1.30256651e+01 4.12329474e+00\n",
      " 7.52425579e-01 1.61936036e+02 2.75555798e+03 2.30432045e-04\n",
      " 1.54654736e-01 4.55848775e+00 4.64298922e+01]\n",
      "P-Values\n",
      "[4.64500416e-02 8.15035154e-03 2.31394417e-02 5.31804675e-01\n",
      " 9.79968040e-01 3.82728810e-33 0.00000000e+00 1.00000000e+00\n",
      " 9.99526491e-01 4.72096321e-01 7.42403757e-09]\n"
     ]
    }
   ],
   "source": [
    "chi2_sklearn, pvalue_sklearn = chi2(X, y)\n",
    "print(\"Chi-Squared Statistic\")\n",
    "print(chi2_sklearn)\n",
    "\n",
    "print(\"P-Values\")\n",
    "print(pvalue_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Rank features descendingly by χ² statistic\n",
    "\n",
    "Optionally, then sort these indices so they remain relative order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 5, 6, 10]\n"
     ]
    }
   ],
   "source": [
    "support = sorted(np.argsort(-chi_squared)[:5])\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Select these features and transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X[:, support]\n",
    "np.allclose(X_new, X_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on the p-value\n",
    "\n",
    "sklearn not only has provided the **χ² statistic** but also **p-value**,\n",
    "which is very useful because p-value could tell you quantitively how probable each feature is relevant,\n",
    "which in turn helps you decide how many and which features are worthwhile to retain.\n",
    "\n",
    "Here's part of a **χ² distribution** table.\n",
    "In our dataset, we have (#classes - 1) = 5 degrees of freedom (d.o.f.).\n",
    "\n",
    "<table cellspacing=\"2\" cellpadding=\"3\" border=\"1\" align=\"center\">\n",
    "<tbody>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">df</td><td bgcolor=\"#009bff\">0.995</td><td bgcolor=\"#009bff\">0.99</td><td bgcolor=\"#009bff\">0.975</td><td bgcolor=\"#009bff\">0.95</td><td bgcolor=\"#009bff\">0.9</td><td bgcolor=\"#009bff\">0.1</td><td bgcolor=\"#009bff\">0.05</td><td bgcolor=\"#009bff\">0.025</td><td bgcolor=\"#009bff\">0.01</td><td bgcolor=\"#009bff\">0.005</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">1</td><td bgcolor=\"#cdebfa\">0</td><td bgcolor=\"#edfbfa\">0</td><td bgcolor=\"#cdebfa\">0.001</td><td bgcolor=\"#edfbfa\">0.004</td><td bgcolor=\"#cdebfa\">0.016</td><td bgcolor=\"#edfbfa\">2.706</td><td bgcolor=\"#cdebfa\">3.841</td><td bgcolor=\"#edfbfa\">5.024</td><td bgcolor=\"#cdebfa\">6.635</td><td bgcolor=\"#edfbfa\">7.879</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">2</td><td bgcolor=\"#edfbfa\">0.01</td><td bgcolor=\"#cdebfa\">0.02</td><td bgcolor=\"#edfbfa\">0.051</td><td bgcolor=\"#cdebfa\">0.103</td><td bgcolor=\"#edfbfa\">0.211</td><td bgcolor=\"#cdebfa\">4.605</td><td bgcolor=\"#edfbfa\">5.991</td><td bgcolor=\"#cdebfa\">7.378</td><td bgcolor=\"#edfbfa\">9.21</td><td bgcolor=\"#cdebfa\">10.597</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">3</td><td bgcolor=\"#cdebfa\">0.072</td><td bgcolor=\"#edfbfa\">0.115</td><td bgcolor=\"#cdebfa\">0.216</td><td bgcolor=\"#edfbfa\">0.352</td><td bgcolor=\"#cdebfa\">0.584</td><td bgcolor=\"#edfbfa\">6.251</td><td bgcolor=\"#cdebfa\">7.815</td><td bgcolor=\"#edfbfa\">9.348</td><td bgcolor=\"#cdebfa\">11.345</td><td bgcolor=\"#edfbfa\">12.838</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">4</td><td bgcolor=\"#edfbfa\">0.207</td><td bgcolor=\"#cdebfa\">0.297</td><td bgcolor=\"#edfbfa\">0.484</td><td bgcolor=\"#cdebfa\">0.711</td><td bgcolor=\"#edfbfa\">1.064</td><td bgcolor=\"#cdebfa\">7.779</td><td bgcolor=\"#edfbfa\">9.488</td><td bgcolor=\"#cdebfa\">11.143</td><td bgcolor=\"#edfbfa\">13.277</td><td bgcolor=\"#cdebfa\">14.86</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">5</td><td bgcolor=\"#cdebfa\">0.412</td><td bgcolor=\"#edfbfa\">0.554</td><td bgcolor=\"#cdebfa\">0.831</td><td bgcolor=\"#edfbfa\">1.145</td><td bgcolor=\"#cdebfa\">1.61</td><td bgcolor=\"#edfbfa\">9.236</td><td bgcolor=\"#cdebfa\">11.07</td><td bgcolor=\"#edfbfa\">12.833</td><td bgcolor=\"#cdebfa\">15.086</td><td bgcolor=\"#edfbfa\">16.75</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">6</td><td bgcolor=\"#edfbfa\">0.676</td><td bgcolor=\"#cdebfa\">0.872</td><td bgcolor=\"#edfbfa\">1.237</td><td bgcolor=\"#cdebfa\">1.635</td><td bgcolor=\"#edfbfa\">2.204</td><td bgcolor=\"#cdebfa\">10.645</td><td bgcolor=\"#edfbfa\">12.592</td><td bgcolor=\"#cdebfa\">14.449</td><td bgcolor=\"#edfbfa\">16.812</td><td bgcolor=\"#cdebfa\">18.548</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">7</td><td bgcolor=\"#cdebfa\">0.989</td><td bgcolor=\"#edfbfa\">1.239</td><td bgcolor=\"#cdebfa\">1.69</td><td bgcolor=\"#edfbfa\">2.167</td><td bgcolor=\"#cdebfa\">2.833</td><td bgcolor=\"#edfbfa\">12.017</td><td bgcolor=\"#cdebfa\">14.067</td><td bgcolor=\"#edfbfa\">16.013</td><td bgcolor=\"#cdebfa\">18.475</td><td bgcolor=\"#edfbfa\">20.278</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">8</td><td bgcolor=\"#edfbfa\">1.344</td><td bgcolor=\"#cdebfa\">1.646</td><td bgcolor=\"#edfbfa\">2.18</td><td bgcolor=\"#cdebfa\">2.733</td><td bgcolor=\"#edfbfa\">3.49</td><td bgcolor=\"#cdebfa\">13.362</td><td bgcolor=\"#edfbfa\">15.507</td><td bgcolor=\"#cdebfa\">17.535</td><td bgcolor=\"#edfbfa\">20.09</td><td bgcolor=\"#cdebfa\">21.955</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">9</td><td bgcolor=\"#cdebfa\">1.735</td><td bgcolor=\"#edfbfa\">2.088</td><td bgcolor=\"#cdebfa\">2.7</td><td bgcolor=\"#edfbfa\">3.325</td><td bgcolor=\"#cdebfa\">4.168</td><td bgcolor=\"#edfbfa\">14.684</td><td bgcolor=\"#cdebfa\">16.919</td><td bgcolor=\"#edfbfa\">19.023</td><td bgcolor=\"#cdebfa\">21.666</td><td bgcolor=\"#edfbfa\">23.589</td></tr>\n",
    "<tr class=\"col1\"><td bgcolor=\"#009bff\">10</td><td bgcolor=\"#edfbfa\">2.156</td><td bgcolor=\"#cdebfa\">2.558</td><td bgcolor=\"#edfbfa\">3.247</td><td bgcolor=\"#cdebfa\">3.94</td><td bgcolor=\"#edfbfa\">4.865</td><td bgcolor=\"#cdebfa\">15.989</td><td bgcolor=\"#edfbfa\">18.307</td><td bgcolor=\"#cdebfa\">20.483</td><td bgcolor=\"#edfbfa\">23.209</td><td bgcolor=\"#cdebfa\">25.188</td></tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our null hypothesis is that a class labels is independent of a feature. \n",
    "Since here we used **χ² statistic** for testing what the null hypothesis has claimed : **independence between features and labels**.\n",
    "We can choose `p-value` $>2.5\\%$ as the critical level at which the null hypothesis **can not be rejected**,\n",
    "and reject the hypothesis otherwise.\n",
    "This is effectively saying that, if the feature is more that 10% chance to be independent of the class, then it is not a good predictor.\n",
    "The alternative hypothesis is, therefore, that the feature is probable predictor of the class, i.e., the class is dependent on the feature.\n",
    "The following table would summarize our conclusion based on this criterion:\n",
    "\n",
    "<table>\n",
    "<tr><td><strong>feature idx</strong></td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td></tr>\n",
    "<tr><td><strong>χ²</strong></td><td>11.3</td><td>15.6</td><td>13.0</td><td>4.12</td><td>0.752</td><td>162</td><td>2.76e+03</td><td>2.30e-04</td><td>0.155</td><td>4.56</td><td>46.4</td></tr>\n",
    "<tr><td><strong>p-value</strong></td><td>~5%</td><td>~0.5%</td><td>~1%</td><td>90%~95%</td><td>100%</td><td>0%</td><td>0%</td><td>100%</td><td>100%</td><td>90%~95%</td><td>0%</td></tr>\n",
    "<tr><td><strong>interpretation</strong></td><td>independent</td><td><strong>dependent</strong></td><td><strong>dependent</strong></td><td>independent</td><td>independent</td><td><strong>dependent</strong></td><td><strong>dependent</strong></td><td>independent</td><td>independent</td><td>independent</td><td><strong>dependent</strong></td></tr>\n",
    "</table>\n",
    "\n",
    "So we should select feature 1, 2, 5, 6 and 10.\n",
    "\n",
    "<!-- future work:\n",
    "sklearn learn can help make this process more precise and easy than looking up the **χ² distribution** table.\n",
    "Following cells prints the all the p-values:\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please review the documentation of the chi2 function from `sklearn.feature_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function chi2 in module sklearn.feature_selection._univariate_selection:\n",
      "\n",
      "chi2(X, y)\n",
      "    Compute chi-squared stats between each non-negative feature and class.\n",
      "    \n",
      "    This score can be used to select the n_features features with the\n",
      "    highest values for the test chi-squared statistic from X, which must\n",
      "    contain only non-negative features such as booleans or frequencies\n",
      "    (e.g., term counts in document classification), relative to the classes.\n",
      "    \n",
      "    Recall that the chi-square test measures dependence between stochastic\n",
      "    variables, so using this function \"weeds out\" the features that are the\n",
      "    most likely to be independent of class and therefore irrelevant for\n",
      "    classification.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "        Sample vectors.\n",
      "    \n",
      "    y : array-like of shape (n_samples,)\n",
      "        Target vector (class labels).\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    chi2 : array, shape = (n_features,)\n",
      "        chi2 statistics of each feature.\n",
      "    pval : array, shape = (n_features,)\n",
      "        p-values of each feature.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Complexity of this algorithm is O(n_classes * n_features).\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    f_classif : ANOVA F-value between label/feature for classification tasks.\n",
      "    f_regression : F-value between label/feature for regression tasks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "--- \n",
    "\n",
    "Here's how to generate χ² table, which may come in handy now and then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.995</th>\n",
       "      <th>0.990</th>\n",
       "      <th>0.975</th>\n",
       "      <th>0.950</th>\n",
       "      <th>0.900</th>\n",
       "      <th>0.100</th>\n",
       "      <th>0.050</th>\n",
       "      <th>0.025</th>\n",
       "      <th>0.010</th>\n",
       "      <th>0.005</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.411742</td>\n",
       "      <td>0.554298</td>\n",
       "      <td>0.831212</td>\n",
       "      <td>1.145476</td>\n",
       "      <td>1.610308</td>\n",
       "      <td>9.236357</td>\n",
       "      <td>11.070498</td>\n",
       "      <td>12.832502</td>\n",
       "      <td>15.086272</td>\n",
       "      <td>16.749602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.675727</td>\n",
       "      <td>0.872090</td>\n",
       "      <td>1.237344</td>\n",
       "      <td>1.635383</td>\n",
       "      <td>2.204131</td>\n",
       "      <td>10.644641</td>\n",
       "      <td>12.591587</td>\n",
       "      <td>14.449375</td>\n",
       "      <td>16.811894</td>\n",
       "      <td>18.547584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.989256</td>\n",
       "      <td>1.239042</td>\n",
       "      <td>1.689869</td>\n",
       "      <td>2.167350</td>\n",
       "      <td>2.833107</td>\n",
       "      <td>12.017037</td>\n",
       "      <td>14.067140</td>\n",
       "      <td>16.012764</td>\n",
       "      <td>18.475307</td>\n",
       "      <td>20.277740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.344413</td>\n",
       "      <td>1.646497</td>\n",
       "      <td>2.179731</td>\n",
       "      <td>2.732637</td>\n",
       "      <td>3.489539</td>\n",
       "      <td>13.361566</td>\n",
       "      <td>15.507313</td>\n",
       "      <td>17.534546</td>\n",
       "      <td>20.090235</td>\n",
       "      <td>21.954955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.734933</td>\n",
       "      <td>2.087901</td>\n",
       "      <td>2.700389</td>\n",
       "      <td>3.325113</td>\n",
       "      <td>4.168159</td>\n",
       "      <td>14.683657</td>\n",
       "      <td>16.918978</td>\n",
       "      <td>19.022768</td>\n",
       "      <td>21.665994</td>\n",
       "      <td>23.589351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.155856</td>\n",
       "      <td>2.558212</td>\n",
       "      <td>3.246973</td>\n",
       "      <td>3.940299</td>\n",
       "      <td>4.865182</td>\n",
       "      <td>15.987179</td>\n",
       "      <td>18.307038</td>\n",
       "      <td>20.483177</td>\n",
       "      <td>23.209251</td>\n",
       "      <td>25.188180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.603222</td>\n",
       "      <td>3.053484</td>\n",
       "      <td>3.815748</td>\n",
       "      <td>4.574813</td>\n",
       "      <td>5.577785</td>\n",
       "      <td>17.275009</td>\n",
       "      <td>19.675138</td>\n",
       "      <td>21.920049</td>\n",
       "      <td>24.724970</td>\n",
       "      <td>26.756849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0.995     0.990     0.975     0.950     0.900      0.100      0.050  \\\n",
       "5   0.411742  0.554298  0.831212  1.145476  1.610308   9.236357  11.070498   \n",
       "6   0.675727  0.872090  1.237344  1.635383  2.204131  10.644641  12.591587   \n",
       "7   0.989256  1.239042  1.689869  2.167350  2.833107  12.017037  14.067140   \n",
       "8   1.344413  1.646497  2.179731  2.732637  3.489539  13.361566  15.507313   \n",
       "9   1.734933  2.087901  2.700389  3.325113  4.168159  14.683657  16.918978   \n",
       "10  2.155856  2.558212  3.246973  3.940299  4.865182  15.987179  18.307038   \n",
       "11  2.603222  3.053484  3.815748  4.574813  5.577785  17.275009  19.675138   \n",
       "\n",
       "        0.025      0.010      0.005  \n",
       "5   12.832502  15.086272  16.749602  \n",
       "6   14.449375  16.811894  18.547584  \n",
       "7   16.012764  18.475307  20.277740  \n",
       "8   17.534546  20.090235  21.954955  \n",
       "9   19.022768  21.665994  23.589351  \n",
       "10  20.483177  23.209251  25.188180  \n",
       "11  21.920049  24.724970  26.756849  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chi2_table(degree_of_freedoms):\n",
    "    from scipy.stats import chi2 as chi2_distribution\n",
    "    pvalue = np.array([0.995, 0.99, 0.975, 0.95, 0.90, 0.10, 0.05, 0.025, 0.01, 0.005])\n",
    "    return pd.DataFrame(chi2_distribution.isf(\n",
    "            # isf(p) = inverse(1-cdf)(p) which takes p-value returns chi square value\n",
    "            #     where cdf is short for cumulative distribution function\n",
    "        pvalue, np.expand_dims(degree_of_freedoms, 1)),\n",
    "        columns = pvalue, index = degree_of_freedoms)\n",
    "\n",
    "chi2_table(range(5, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### B. ANOVA F-value based feature selection\n",
    "\n",
    "The following cell shows the usage of a feature selector based on ANOVA F-value and Pearson's correlation.\n",
    "This calculates Pearson's correlation of each feature vs the label in order to determine which feature is more relevant, \n",
    "one at a time, then selects features according to the ANOVA F-value derived from Pearson's correlation.\n",
    "\n",
    "See also: https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.pearsonr.html\n",
    "\n",
    "**Note:** You previously saw the ANOVA and MANOVA within the 8610 (Stat/Math) course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score [2.49600375e+01 2.87444450e+02 8.62577262e+01 3.01183699e-01\n",
      " 2.69856084e+01 4.10850227e+00 5.66578176e+01 5.04052231e+01\n",
      " 5.34046221e+00 1.07740433e+02 4.68267011e+02]\n",
      "Selected indices [ 1  2  6  9 10]\n"
     ]
    }
   ],
   "source": [
    "selector = SelectKBest(f_regression, k=5)\n",
    "selector.fit(X, y)\n",
    "print('score', selector.scores_)\n",
    "print('Selected indices', selector.get_support(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will take a closer look at this procedure to gain a more solid understanding.\n",
    "\n",
    "**Note:** _You do NOT need to learn by heart the details of the computation to work on practices._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Compute cross correlation\n",
    "\n",
    "$$r_j = \\frac{\\sigma_{X_j y}}{\\sigma_{X_j} \\sigma_y} = \\frac{(y-\\bar y)^T (X_j-\\bar {X_j})}{\\lVert X_j-\\bar {X_j}\\rVert \\cdot \\lVert y-\\bar y\\rVert}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.12405165 -0.39055778  0.22637251  0.01373164 -0.12890656 -0.05065606\n",
      " -0.18510029 -0.17491923 -0.05773139  0.25139708  0.47616632]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "X_centered = scale(X.astype('float'), with_std = False)\n",
    "y_centered = scale(y.astype('float'), with_std = False)\n",
    "corr = np.dot(y_centered, X_centered) / np.linalg.norm(X_centered, axis = 0) / np.linalg.norm(y_centered)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12405164911322428, -0.3905577802640073, 0.2263725143180414, 0.013731637340066303, -0.12890655993005273, -0.05065605724427639, -0.18510028892653774, -0.17491922778334879, -0.05773139120538215, 0.2513970790692614, 0.47616632400113595]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "print([pearsonr(X[:,i], y)[0] for i in range(X.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Compute ANOVA F-value\n",
    "\n",
    "This is a F-test for correlation coefficients.\n",
    "Read more [here](https://onlinecourses.science.psu.edu/stat501/lesson/2/2.6).\n",
    "Similar to the way that χ² test is provided by sklearn, [f_regression()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression) also supplied p-value for quantitatively assessing how relevant each feature is.\n",
    "\n",
    "$$ F = t^2 = \\left( \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}} \\right) ^2 = \\frac{r^2/1 }{(1-r^2)/(n-2)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.49600375e+01 2.87444450e+02 8.62577262e+01 3.01183699e-01\n",
      " 2.69856084e+01 4.10850227e+00 5.66578176e+01 5.04052231e+01\n",
      " 5.34046221e+00 1.07740433e+02 4.68267011e+02]\n"
     ]
    }
   ],
   "source": [
    "corr2 = corr ** 2\n",
    "Fvalue = corr2 / (1 - corr2) * (y.shape[0] - 2)\n",
    "print(Fvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to f_regression()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.49600375e+01 2.87444450e+02 8.62577262e+01 3.01183699e-01\n",
      " 2.69856084e+01 4.10850227e+00 5.66578176e+01 5.04052231e+01\n",
      " 5.34046221e+00 1.07740433e+02 4.68267011e+02]\n"
     ]
    }
   ],
   "source": [
    "Fvalue_sklearn, pvalue2_sklearn = f_regression(X,y)\n",
    "print(Fvalue_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Select these features and transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 6, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "support = sorted(np.argsort(-Fvalue)[:5])\n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on F-value and p-value\n",
    "\n",
    "F-test can be used as hypothesis testing for a ratio of two χ² distributions.\n",
    "The F-test for correlation coefficients is derived from testing a ratio between regression sum square (SSR)\n",
    "and error sum square (SSE).\n",
    "\n",
    "$$ SSR = \\sum _{i=1}^n {\\left(\\hat {y_i} - \\bar y \\right)^2}$$\n",
    "\n",
    "$$ SSE = \\sum _{i=1}^n {\\left(\\hat {y_i} - y_i \\right)^2}$$\n",
    "\n",
    "$$ F  = \\frac{SSR/(v-1) }{SSE/(n-2)} $$\n",
    "\n",
    "With correlation coefficients, this is \n",
    "\n",
    "$$ F = \\frac{r^2/1 }{(1-r^2)/(n-2)} $$\n",
    "\n",
    "with degree of freedom 1 and (n-2) respectively.\n",
    "\n",
    "Its null hypothesis claims r = 0, i.e. the independence of each feature and label.\n",
    "We could theoretically make hypothesis testing using an F-distribution table.\n",
    "\n",
    "Here's how to generate F table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.863458</td>\n",
       "      <td>49.500000</td>\n",
       "      <td>53.593245</td>\n",
       "      <td>55.832961</td>\n",
       "      <td>57.240077</td>\n",
       "      <td>58.204416</td>\n",
       "      <td>58.905953</td>\n",
       "      <td>59.438981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.526316</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.161790</td>\n",
       "      <td>9.243416</td>\n",
       "      <td>9.292626</td>\n",
       "      <td>9.325530</td>\n",
       "      <td>9.349081</td>\n",
       "      <td>9.366770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.538319</td>\n",
       "      <td>5.462383</td>\n",
       "      <td>5.390773</td>\n",
       "      <td>5.342644</td>\n",
       "      <td>5.309157</td>\n",
       "      <td>5.284732</td>\n",
       "      <td>5.266195</td>\n",
       "      <td>5.251671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.544771</td>\n",
       "      <td>4.324555</td>\n",
       "      <td>4.190860</td>\n",
       "      <td>4.107250</td>\n",
       "      <td>4.050579</td>\n",
       "      <td>4.009749</td>\n",
       "      <td>3.978966</td>\n",
       "      <td>3.954940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.060420</td>\n",
       "      <td>3.779716</td>\n",
       "      <td>3.619477</td>\n",
       "      <td>3.520196</td>\n",
       "      <td>3.452982</td>\n",
       "      <td>3.404507</td>\n",
       "      <td>3.367899</td>\n",
       "      <td>3.339276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.775950</td>\n",
       "      <td>3.463304</td>\n",
       "      <td>3.288762</td>\n",
       "      <td>3.180763</td>\n",
       "      <td>3.107512</td>\n",
       "      <td>3.054551</td>\n",
       "      <td>3.014457</td>\n",
       "      <td>2.983036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.589428</td>\n",
       "      <td>3.257442</td>\n",
       "      <td>3.074072</td>\n",
       "      <td>2.960534</td>\n",
       "      <td>2.883344</td>\n",
       "      <td>2.827392</td>\n",
       "      <td>2.784930</td>\n",
       "      <td>2.751580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.457919</td>\n",
       "      <td>3.113118</td>\n",
       "      <td>2.923796</td>\n",
       "      <td>2.806426</td>\n",
       "      <td>2.726447</td>\n",
       "      <td>2.668335</td>\n",
       "      <td>2.624135</td>\n",
       "      <td>2.589349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.360303</td>\n",
       "      <td>3.006452</td>\n",
       "      <td>2.812863</td>\n",
       "      <td>2.692680</td>\n",
       "      <td>2.610613</td>\n",
       "      <td>2.550855</td>\n",
       "      <td>2.505313</td>\n",
       "      <td>2.469406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.285015</td>\n",
       "      <td>2.924466</td>\n",
       "      <td>2.727673</td>\n",
       "      <td>2.605336</td>\n",
       "      <td>2.521641</td>\n",
       "      <td>2.460582</td>\n",
       "      <td>2.413965</td>\n",
       "      <td>2.377150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.880695</td>\n",
       "      <td>2.488716</td>\n",
       "      <td>2.276071</td>\n",
       "      <td>2.142235</td>\n",
       "      <td>2.049246</td>\n",
       "      <td>1.980333</td>\n",
       "      <td>1.926916</td>\n",
       "      <td>1.884121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.808658</td>\n",
       "      <td>2.411955</td>\n",
       "      <td>2.196730</td>\n",
       "      <td>2.060816</td>\n",
       "      <td>1.965999</td>\n",
       "      <td>1.895431</td>\n",
       "      <td>1.840496</td>\n",
       "      <td>1.796300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2.778604</td>\n",
       "      <td>2.380015</td>\n",
       "      <td>2.163735</td>\n",
       "      <td>2.026947</td>\n",
       "      <td>1.931343</td>\n",
       "      <td>1.860049</td>\n",
       "      <td>1.804438</td>\n",
       "      <td>1.759607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2.762115</td>\n",
       "      <td>2.362513</td>\n",
       "      <td>2.145660</td>\n",
       "      <td>2.008390</td>\n",
       "      <td>1.912348</td>\n",
       "      <td>1.840645</td>\n",
       "      <td>1.784650</td>\n",
       "      <td>1.739457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2.751698</td>\n",
       "      <td>2.351464</td>\n",
       "      <td>2.134251</td>\n",
       "      <td>1.996676</td>\n",
       "      <td>1.900354</td>\n",
       "      <td>1.828389</td>\n",
       "      <td>1.772147</td>\n",
       "      <td>1.726719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>2.744520</td>\n",
       "      <td>2.343855</td>\n",
       "      <td>2.126395</td>\n",
       "      <td>1.988609</td>\n",
       "      <td>1.892093</td>\n",
       "      <td>1.819946</td>\n",
       "      <td>1.763531</td>\n",
       "      <td>1.717939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2.739275</td>\n",
       "      <td>2.338296</td>\n",
       "      <td>2.120656</td>\n",
       "      <td>1.982716</td>\n",
       "      <td>1.886057</td>\n",
       "      <td>1.813776</td>\n",
       "      <td>1.757233</td>\n",
       "      <td>1.711520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2.735274</td>\n",
       "      <td>2.334056</td>\n",
       "      <td>2.116279</td>\n",
       "      <td>1.978222</td>\n",
       "      <td>1.881454</td>\n",
       "      <td>1.809070</td>\n",
       "      <td>1.752429</td>\n",
       "      <td>1.706623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2.732121</td>\n",
       "      <td>2.330717</td>\n",
       "      <td>2.112832</td>\n",
       "      <td>1.974681</td>\n",
       "      <td>1.877827</td>\n",
       "      <td>1.805362</td>\n",
       "      <td>1.748644</td>\n",
       "      <td>1.702763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>2.729573</td>\n",
       "      <td>2.328018</td>\n",
       "      <td>2.110046</td>\n",
       "      <td>1.971820</td>\n",
       "      <td>1.874896</td>\n",
       "      <td>1.802365</td>\n",
       "      <td>1.745584</td>\n",
       "      <td>1.699643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2.727471</td>\n",
       "      <td>2.325791</td>\n",
       "      <td>2.107748</td>\n",
       "      <td>1.969460</td>\n",
       "      <td>1.872478</td>\n",
       "      <td>1.799892</td>\n",
       "      <td>1.743059</td>\n",
       "      <td>1.697068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2.725707</td>\n",
       "      <td>2.323924</td>\n",
       "      <td>2.105819</td>\n",
       "      <td>1.967480</td>\n",
       "      <td>1.870450</td>\n",
       "      <td>1.797818</td>\n",
       "      <td>1.740941</td>\n",
       "      <td>1.694908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             1          2          3          4          5          6  \\\n",
       "1    39.863458  49.500000  53.593245  55.832961  57.240077  58.204416   \n",
       "2     8.526316   9.000000   9.161790   9.243416   9.292626   9.325530   \n",
       "3     5.538319   5.462383   5.390773   5.342644   5.309157   5.284732   \n",
       "4     4.544771   4.324555   4.190860   4.107250   4.050579   4.009749   \n",
       "5     4.060420   3.779716   3.619477   3.520196   3.452982   3.404507   \n",
       "6     3.775950   3.463304   3.288762   3.180763   3.107512   3.054551   \n",
       "7     3.589428   3.257442   3.074072   2.960534   2.883344   2.827392   \n",
       "8     3.457919   3.113118   2.923796   2.806426   2.726447   2.668335   \n",
       "9     3.360303   3.006452   2.812863   2.692680   2.610613   2.550855   \n",
       "10    3.285015   2.924466   2.727673   2.605336   2.521641   2.460582   \n",
       "30    2.880695   2.488716   2.276071   2.142235   2.049246   1.980333   \n",
       "50    2.808658   2.411955   2.196730   2.060816   1.965999   1.895431   \n",
       "70    2.778604   2.380015   2.163735   2.026947   1.931343   1.860049   \n",
       "90    2.762115   2.362513   2.145660   2.008390   1.912348   1.840645   \n",
       "110   2.751698   2.351464   2.134251   1.996676   1.900354   1.828389   \n",
       "130   2.744520   2.343855   2.126395   1.988609   1.892093   1.819946   \n",
       "150   2.739275   2.338296   2.120656   1.982716   1.886057   1.813776   \n",
       "170   2.735274   2.334056   2.116279   1.978222   1.881454   1.809070   \n",
       "190   2.732121   2.330717   2.112832   1.974681   1.877827   1.805362   \n",
       "210   2.729573   2.328018   2.110046   1.971820   1.874896   1.802365   \n",
       "230   2.727471   2.325791   2.107748   1.969460   1.872478   1.799892   \n",
       "250   2.725707   2.323924   2.105819   1.967480   1.870450   1.797818   \n",
       "\n",
       "             7          8  \n",
       "1    58.905953  59.438981  \n",
       "2     9.349081   9.366770  \n",
       "3     5.266195   5.251671  \n",
       "4     3.978966   3.954940  \n",
       "5     3.367899   3.339276  \n",
       "6     3.014457   2.983036  \n",
       "7     2.784930   2.751580  \n",
       "8     2.624135   2.589349  \n",
       "9     2.505313   2.469406  \n",
       "10    2.413965   2.377150  \n",
       "30    1.926916   1.884121  \n",
       "50    1.840496   1.796300  \n",
       "70    1.804438   1.759607  \n",
       "90    1.784650   1.739457  \n",
       "110   1.772147   1.726719  \n",
       "130   1.763531   1.717939  \n",
       "150   1.757233   1.711520  \n",
       "170   1.752429   1.706623  \n",
       "190   1.748644   1.702763  \n",
       "210   1.745584   1.699643  \n",
       "230   1.743059   1.697068  \n",
       "250   1.740941   1.694908  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f_table(alpha, v1, v2):\n",
    "    from scipy.stats import f as f_distribution\n",
    "    v1 = np.array(list(v1)); v2 = np.array(list(v2))\n",
    "    return pd.DataFrame(f_distribution.isf(alpha, v1[np.newaxis, ...], v2[..., np.newaxis]),\n",
    "        columns = v1, index = v2)\n",
    "\n",
    "import itertools\n",
    "f_table(0.1, range(1, 9), itertools.chain(range(1,10), range(10, 260, 20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, directly compute p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.49563501e-07, 2.05171481e-59, 4.99129525e-20, 5.83218013e-01,\n",
       "       2.31338265e-07, 4.28339795e-02, 8.62170342e-14, 1.87495665e-12,\n",
       "       2.09627787e-02, 1.80208845e-24, 2.83147697e-91])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import f as f_distribution\n",
    "f_distribution.sf(Fvalue, 1, y.shape[0] - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to those obtained from f_regression()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.49563501e-07, 2.05171481e-59, 4.99129525e-20, 5.83218013e-01,\n",
       "       2.31338265e-07, 4.28339795e-02, 8.62170342e-14, 1.87495663e-12,\n",
       "       2.09627787e-02, 1.80208845e-24, 2.83147697e-91])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue2_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print in percentages perhaps makes it easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., 58.,  0.,  4.,  0.,  0.,  2.,  0.,  0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(pvalue2_sklearn * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "\n",
    "We just went through **χ²** and **F-value** based feature selection in detail because \n",
    "these two different methods work better on classification and regression respectively.\n",
    "The general steps for doing feature selection with sklearn was:\n",
    "\n",
    "1. Choose a feature scoring method\n",
    "2. Initialize a feature selector\n",
    "3. Fit feature selector on the data\n",
    "\n",
    "Other feature selection methods provided by sklearn include:\n",
    "\n",
    "* Classification: chi2, f_classif, mutual_info_classif\n",
    "* Regresssion: f_regression, mutual_info_regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this lab, we learned about:\n",
    "\n",
    "* Wrapper methods\n",
    "    * Forward selection\n",
    "    * Backward elimination\n",
    "    * Recursive feature elimination\n",
    "    \n",
    " \n",
    "* Filter methods\n",
    "    * Pearson's χ²\n",
    "    * ANOVA F-value\n",
    "\n",
    "\n",
    "And once again,\n",
    "1. Wrapper methods are usually computationally expensive\n",
    "2. Greedy algorithms don't necessary provide the optimal solution, which may be good becuase it makes them less prone to overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
