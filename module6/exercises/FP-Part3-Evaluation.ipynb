{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Unbiased Evaluation using a New Test Set\n",
    "\n",
    "In this part, we are given a new test set (`/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`). We can now take advantage of the entire smart sample that we created in Part I. \n",
    "\n",
    "* Retrain a pipeline using the optimal parameters that the pipeline learned. We don't need to repeat GridSearch here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the above\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load smart sample and the best pipeline from Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample_X, Sample_y = joblib.load('sample-data-v1.pkl')\n",
    "Pipe_1 = joblib.load('model_one.pkl')\n",
    "Model_1 = joblib.load('model_one_best.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Retrain a pipeline using the full sampled training data set\n",
    "\n",
    "Use the full sampled training data set to train the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Sample_X, Sample_y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E301)\n",
    "# ----------------------------------\n",
    "\n",
    "mod_1_run = Pipe_1.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.56      0.66      2215\n",
      "           1       0.67      0.87      0.76      2303\n",
      "\n",
      "    accuracy                           0.72      4518\n",
      "   macro avg       0.74      0.72      0.71      4518\n",
      "weighted avg       0.74      0.72      0.71      4518\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing the prediction of this model\n",
    "y_pred = mod_1_run.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So the above is not as good as what was run in part 2. I predict that this is most likely due to not conducting any outlier/anamoly detection to clean up the sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the trained model with the pickle library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mod_1_run.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add code below this comment  \n",
    "# -----------------------------\n",
    "\n",
    "joblib.dump(mod_1_run, 'mod_1_run.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load the Testing Data and evaluate your model\n",
    "\n",
    " * `/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv`\n",
    " \n",
    "* We need to preprocess this test data (follow the steps similar to Part I)\n",
    "* If we have fitted any normalizer/standardizer in Part 2, then we have to transform this test data using the fitted normalizer/standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>...</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <th>local_bo_qty</th>\n",
       "      <th>deck_risk</th>\n",
       "      <th>oe_constraint</th>\n",
       "      <th>ppap_risk</th>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <th>rev_stop</th>\n",
       "      <th>went_on_backorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3285085</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-99.00</td>\n",
       "      <td>-99.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3285131</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-99.00</td>\n",
       "      <td>-99.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3285358</td>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3285517</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3285608</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sku  national_inv  lead_time  in_transit_qty  forecast_3_month  \\\n",
       "0  3285085          62.0        NaN             0.0               0.0   \n",
       "1  3285131           9.0        NaN             0.0               0.0   \n",
       "2  3285358          17.0        8.0             0.0               0.0   \n",
       "3  3285517           9.0        2.0             0.0               0.0   \n",
       "4  3285608           2.0        8.0             0.0               0.0   \n",
       "\n",
       "   forecast_6_month  forecast_9_month  sales_1_month  sales_3_month  \\\n",
       "0               0.0               0.0            0.0            0.0   \n",
       "1               0.0               0.0            0.0            0.0   \n",
       "2               0.0               0.0            0.0            0.0   \n",
       "3               0.0               0.0            0.0            0.0   \n",
       "4               0.0               0.0            0.0            0.0   \n",
       "\n",
       "   sales_6_month  ...  pieces_past_due  perf_6_month_avg perf_12_month_avg  \\\n",
       "0            0.0  ...              0.0            -99.00            -99.00   \n",
       "1            0.0  ...              0.0            -99.00            -99.00   \n",
       "2            0.0  ...              0.0              0.92              0.95   \n",
       "3            0.0  ...              0.0              0.78              0.75   \n",
       "4            0.0  ...              0.0              0.54              0.71   \n",
       "\n",
       "   local_bo_qty  deck_risk  oe_constraint  ppap_risk stop_auto_buy rev_stop  \\\n",
       "0           0.0        Yes             No         No           Yes       No   \n",
       "1           0.0         No             No        Yes            No       No   \n",
       "2           0.0         No             No         No           Yes       No   \n",
       "3           0.0         No             No        Yes           Yes       No   \n",
       "4           0.0         No             No         No           Yes       No   \n",
       "\n",
       "  went_on_backorder  \n",
       "0                No  \n",
       "1                No  \n",
       "2                No  \n",
       "3                No  \n",
       "4                No  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the given test set  (Question #E302)\n",
    "# ----------------------------------\n",
    "\n",
    "new_data = pd.read_csv('/dsa/data/all_datasets/back_order/Kaggle_Test_Dataset_v2.csv')\n",
    "\n",
    "new_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 242076 entries, 0 to 242075\n",
      "Data columns (total 23 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   sku                242076 non-null  object \n",
      " 1   national_inv       242075 non-null  float64\n",
      " 2   lead_time          227351 non-null  float64\n",
      " 3   in_transit_qty     242075 non-null  float64\n",
      " 4   forecast_3_month   242075 non-null  float64\n",
      " 5   forecast_6_month   242075 non-null  float64\n",
      " 6   forecast_9_month   242075 non-null  float64\n",
      " 7   sales_1_month      242075 non-null  float64\n",
      " 8   sales_3_month      242075 non-null  float64\n",
      " 9   sales_6_month      242075 non-null  float64\n",
      " 10  sales_9_month      242075 non-null  float64\n",
      " 11  min_bank           242075 non-null  float64\n",
      " 12  potential_issue    242075 non-null  object \n",
      " 13  pieces_past_due    242075 non-null  float64\n",
      " 14  perf_6_month_avg   242075 non-null  float64\n",
      " 15  perf_12_month_avg  242075 non-null  float64\n",
      " 16  local_bo_qty       242075 non-null  float64\n",
      " 17  deck_risk          242075 non-null  object \n",
      " 18  oe_constraint      242075 non-null  object \n",
      " 19  ppap_risk          242075 non-null  object \n",
      " 20  stop_auto_buy      242075 non-null  object \n",
      " 21  rev_stop           242075 non-null  object \n",
      " 22  went_on_backorder  242075 non-null  object \n",
      "dtypes: float64(15), object(8)\n",
      "memory usage: 42.5+ MB\n"
     ]
    }
   ],
   "source": [
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop same columns as before\n",
    "\n",
    "new_data = new_data.drop(['sku', 'lead_time', 'in_transit_qty','min_bank', 'local_bo_qty'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']\n",
      "potential_issue ['No' 'Yes' nan]\n",
      "deck_risk ['Yes' 'No' nan]\n",
      "oe_constraint ['No' 'Yes' nan]\n",
      "ppap_risk ['No' 'Yes' nan]\n",
      "stop_auto_buy ['Yes' 'No' nan]\n",
      "rev_stop ['No' 'Yes' nan]\n",
      "went_on_backorder ['No' 'Yes' nan]\n"
     ]
    }
   ],
   "source": [
    "# Same way to confirm yes/no columns\n",
    "\n",
    "yes_no_columns = list(filter(lambda i: new_data[i].dtype!=np.float64, new_data.columns))\n",
    "print(yes_no_columns)\n",
    "\n",
    "# Add code below this comment  (Question #E102)\n",
    "# ----------------------------------\n",
    "\n",
    "print('potential_issue', new_data['potential_issue'].unique())\n",
    "print('deck_risk', new_data['deck_risk'].unique())\n",
    "print('oe_constraint', new_data['oe_constraint'].unique())\n",
    "print('ppap_risk', new_data['ppap_risk'].unique())\n",
    "print('stop_auto_buy', new_data['stop_auto_buy'].unique())\n",
    "print('rev_stop', new_data['rev_stop'].unique())\n",
    "print('went_on_backorder', new_data['went_on_backorder'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing values of potential_issue with No\n",
      "Filling missing values of deck_risk with No\n",
      "Filling missing values of oe_constraint with No\n",
      "Filling missing values of ppap_risk with No\n",
      "Filling missing values of stop_auto_buy with Yes\n",
      "Filling missing values of rev_stop with No\n",
      "Filling missing values of went_on_backorder with No\n"
     ]
    }
   ],
   "source": [
    "# Preprocess filling with mode and converting to binary\n",
    "\n",
    "for column_name in yes_no_columns:\n",
    "    mode = new_data[column_name].apply(str).mode()[0]\n",
    "    print('Filling missing values of {} with {}'.format(column_name, mode))\n",
    "    new_data[column_name].fillna(mode, inplace=True)\n",
    "    \n",
    "for column_name in yes_no_columns:\n",
    "    new_data[column_name] = new_data[column_name].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA\n",
    "\n",
    "new_data = new_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now predict and evaluate with the preprocessed test set. It would be interesting to see the performance with and without outliers removal from the test set. We can report confusion matrix, precision, recall, f1-score, accuracy, and other measures (if any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code below this comment  (Question #E303)\n",
    "# ----------------------------------\n",
    "\n",
    "#Pipe_2 = joblib.load('model_one.pkl')\n",
    "\n",
    "# Loading the model fresh into Model_2\n",
    "\n",
    "Model_2 = joblib.load('mod_1_run.pkl')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# Commenting out the split and will just run the full portion of the data on the model\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(new_data.iloc[:,:-1], new_data.iloc[:,-1:], test_size = 0.2)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "#----------The below code is old as I played around with the model------------------------------\n",
    "\n",
    "# Fitting the pipe to the new data - This was done for practice. Fitting the above model below\n",
    "# model_3 = Pipe_2.fit(X_train, y_train)\n",
    "\n",
    "# y_pred_2 = model_3.predict(X_test)\n",
    "\n",
    "# Fitting above model with the new data\n",
    "\n",
    "# mod_2_run = Model_2.fit(X_train, y_train)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "\n",
    "y_pred_2 = Model_2.predict(new_data.iloc[:,:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.54      0.70    239387\n",
      "           1       0.02      0.86      0.04      2688\n",
      "\n",
      "    accuracy                           0.55    242075\n",
      "   macro avg       0.51      0.70      0.37    242075\n",
      "weighted avg       0.99      0.55      0.70    242075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "\n",
    "#y_pred = mod_1_run.predict(X_test)\n",
    "\n",
    "print(classification_report(new_data.iloc[:,-1:], y_pred_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[129847, 109540],\n",
       "       [   377,   2311]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(new_data.iloc[:,-1:], y_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above confusion matrix and the classification report this model is not the best as-is at predicting back orders. It did end up predicting 86% of the back-order items correctly, but it also classified nearly 110,000 non-back order items as at risk for back order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    239387\n",
       "1      2688\n",
       "Name: went_on_backorder, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ran to evaluate the true number of predictions compared to what actually existed in the data\n",
    "\n",
    "new_data.went_on_backorder.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to run the same after cleaning up and removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier with the elliptic envelope function code again\n",
    "\n",
    "def elliptic_envelope_session(X, y):\n",
    "    # Fit envelope\n",
    "    envelope = EllipticEnvelope(support_fraction=1, contamination=0.2).fit(X)\n",
    "\n",
    "    # Create an boolean indexing array to pick up outliers\n",
    "    outliers = envelope.predict(X)==-1\n",
    "\n",
    "    # Re-slice X,y into a cleaned dataset with outliers excluded\n",
    "    X_clean = X[~outliers]\n",
    "    y_clean = y[~outliers]\n",
    "    return X_clean, y_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_data.iloc[:,:-1]\n",
    "y = new_data.iloc[:,-1:]\n",
    "\n",
    "X_in, y_in = elliptic_envelope_session(X, y)\n",
    "\n",
    "y_pred_3 = Model_2.predict(X_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.47      0.64    191492\n",
      "           1       0.02      0.89      0.04      2165\n",
      "\n",
      "    accuracy                           0.48    193657\n",
      "   macro avg       0.51      0.68      0.34    193657\n",
      "weighted avg       0.99      0.48      0.63    193657\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_in, y_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 90140, 101352],\n",
       "       [   245,   1920]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_in, y_pred_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, this model appears to have run better without the outlier detection code and keeping the dataset whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write a summary of your processing and an analysis of the model performance  \n",
    "# (Question #E304)\n",
    "# ----------------------------------\n",
    "\n",
    "For this model, I started by evaluating what I believed would be irrelevant data to predict SKU backorder from a larger dataset. This process was more based on intuition and experience and less on consecutive runs of a model, though given more time and computing power I would like to have done more here. \n",
    "\n",
    "That being said, after reducing the features in the data set I created a more manageable dataset size for the computing hardware available by downsampling so that the majority class (did not go on back-order) was represented an equal amount of times as the minority class (went on back-order). This gave me an equal sample size of 11,293 instances from both classes. \n",
    "\n",
    "After downsampling was conducted I moved on to run 3 pipelines using a variety of techniques to determine the best pipeline in phase 2. The top pipeline contained the following features\n",
    "\n",
    "- Elliptic Envelope : Used for outlier detection and removal\n",
    "- PCA to reduce dimensionality\n",
    "- Support Vector Classifier: main engine used to create the predictions of the model\n",
    "\n",
    "This pipeline produced the best results of all the ones I tested, yielding an f-1 score of 0.80, which lead me to want to save and continue forward with this model. \n",
    "\n",
    "After bringing this saved model forward and retraining on the full data set the f-1 score dropped from 0.80 to 0.71, which still seemed fairly accurate for a real-world dataset. \n",
    "\n",
    "Next, this model was used on a new dataset and this is where it turned on me. After running predictions based on the fully trained set the best score I was able to produce was 0.37 macro average for f-1. As noted above, it captured 86% of the back-ordered items correctly, but also miss-classified nearly 110,000 non-back ordered items which doesn't bode well for such a model. Therefore, this model was more apt to disproportionatly classify SKUs in the minority class than the majority.\n",
    "\n",
    "Overall my model ended up not being as fine-tuned as I believe it could have been. There are several factors that I believe could have led to this:\n",
    "\n",
    "- Removing features that would have been meaningful from the initial dataset in part 1.\n",
    "- Additional testing on combinations of pipeline elements. Potentially seeing if PCA mixed with Gaussian NB would have produced more favorable results.\n",
    "- Potentially more cleaning on the data used, however this may impact the results as we would want to use the full dataset that we had available in a real-world situation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflect\n",
    "\n",
    "Imagine you are data scientist that has been tasked with developing a system to save your \n",
    "company money by predicting and preventing back orders of parts in the supply chain.\n",
    "\n",
    "Write a **brief summary** for \"management\" that details your findings, \n",
    "your level of certainty and trust in the models, \n",
    "and recommendations for operationalizing these models for the business."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write your answer here:  \n",
    "# (Question #E305)\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "The goal of this model was to predict whether or not a specific SKU would go on back order based on a variety of factors. This model was built using several select features including:\n",
    "\n",
    "- Elliptic Envelope : Used for outlier detection and removal\n",
    "- PCA : Used to reduce dimensionality\n",
    "- Support Vector Classifier: main engine used to create the predictions of the model\n",
    "\n",
    "After selecting the best model from the above elements through Grid Search, we came upon a model that was fairly accurate when processing the trian/test data. This initial model produced a precision of 0.81 and an f1-score of 0.80 which led me to believe it was the right model to move forward with. \n",
    "\n",
    "When conducting additional predictions from the model on a new set of data, the model performance dropped significantly from the above scores, now producing a precision 0.51 and an f1-score of 0.37. \n",
    "\n",
    "Seeing this swing in scores there are a few recommendations I would make to improve model performance:\n",
    "\n",
    "- Retaining features that were removed earlier on in the development process. \n",
    "- Testing additional variations of model components (dimensionality techniques, classifier algorithms)\n",
    "- Additional Hyperparameter tunings to see if we need to increase the range we tested on in the pipelines. \n",
    "\n",
    "Though this model could be used to guide direction for sales on SKUs, I would not put this model into deployment as it stands today to be able to confidently predict an item to go on back order without additional testing and training of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook!\n",
    "## Then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
